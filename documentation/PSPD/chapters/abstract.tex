\chapter*{Abstract}

% Cloud computing is in a steady rise for the past few years due to its scalability and ease of use. With this change, a new programming paradigm called cloud-native was born. Cloud-native applications are often developed as a set of stand-alone microservices yet could depend on each other to provide a unified experience. Even though their microservices bring a lot to the table when it comes to the flexibility it could be a nightmare to operate in production. Specifically when operating is large system with 100s of microservices talking to each other, smallest problem could result in failures all around the system.

Cloud computing is a steady rise for the past few years due to its scalability and ease of use. With this change, a new programming paradigm called cloud-native was born. Cloud-native applications are often developed as a set of stand-alone microservices yet could depend on each other to provide a unified experience. 

This helps different teams to work on different services which increases the development velocity. This works well for medium to large companies but over time when this mesh of services could become very complicated to a point where it's very difficult for one person to understand the whole system. When the system consists of 1000s of individual services talking and depending on each other, the network layer of that system becomes chaotic and failure in a single point can create a ripple effect across the system. When something like that happens it's really difficult to zero in on the exact point of failure quickly.

The forces of this project are two-fold. First, the authors introduce a robust Kubernetes native toolkit that helps both researchers and developers collect and process service telemetry data with zero instrumentation. This toolkit is designed to be flexible and modular in a way so that future researchers can extend as they fit. Secondly, the authors proposed a novel way of detecting anomalies by encoding raw metric data into an image-like structure and using a convolutional autoencoder to learn the general data distribution for each service and detecting outliers. Finally, a weighted graph was used along with anomaly scores calculated prior to finding out possible root course for any system anomalies.