\chapter*{Abstract}

% Cloud computing is in a steady rise for the past few years due to its scalability and ease of use. With this change, a new programming paradigm called cloud-native was born. Cloud-native applications are often developed as a set of stand-alone microservices yet could depend on each other to provide a unified experience. Even though their microservices bring a lot to the table when it comes to the flexibility it could be a nightmare to operate in production. Specifically when operating is large system with 100s of microservices talking to each other, smallest problem could result in failures all around the system.

Cloud computing is a steady rise for the past few years due to its scalability and ease of use. With this change, a new programming paradigm called cloud-native was born. Cloud-native applications are often developed as a set of stand-alone microservices yet, it could depend on each other to provide a unified experience. 

This helps different teams to work on different services which increases the development velocity. This works well for medium to large companies but over time this mesh of services could become very complicated to a point where it's very difficult for a single person to understand the entire system. When the system consists of thousands of individual services talking and depending on each other, the network layer of that system becomes chaotic. A failure in a single point could create a ripple effect across the entire system. When something like that happens it could take a considerable amount of time to zero in on the exact point of the failure.

The focus of this project are in two-folds. First, the authors introduce a robust Kubernetes native toolkit that helps both researchers and developers collect and process service telemetry data with zero instrumentation. This toolkit is designed to be flexible and modular in a way so that the future researchers can extend as they fit. Secondly, the authors proposed a novel way of detecting anomalies by encoding raw metric data into an image-like structure and using a convolutional autoencoder to learn the general data distribution for each service and detecting outliers. Finally, a weighted graph was used along with anomaly scores calculated prior to finding out possible root cause for any system anomalies.
\newline
\newline
\textbf{Keywords}:
AIOps, Monitoring, Disaster Recovery, eBPF
\newline
\textbf{Subject Descriptors}:
•Computing methodologies \~\ Machine learning \~\ Learning paradigms \~\ Unsupervised learning \~\ Anomaly detection •Computer systems organization \~\ Architectures \~\ Distributed architectures \~\ Cloud computing