\section{Problem Domain}

\subsection{Cloud Computing}
With the emergence of Infrastructure as a Service (IaaS), such as Amazon Web Services (AWS) and Google Cloud Platform (GCP), there is a big surge in organisations trying to outsource their computing needs to third parties \citep{rimol_2021}. This is mainly due to the elasticity given by all cloud providers. Users can easily scale up and down their infrastructure in minutes without making any commitments. All major cloud providers bill customers on the "what you use is what you pay" model. Since the cloud provider manages the entire underlying infrastructure, customers do not have to worry about problems such as hardware failures. In contrast in a self-hosted setting if the user wanted one extra GB of memory than what's available it requires a lot of effort and it costs a lot to fulfill that requirement.

\subsection{Cloud-Native Applications}
During the 90s and early 2000s, all the applications were made as a big monolith from a single code base \citep{LessonsF52:online}. Most of them were shipped as single binary. Since those days the applications were fairly simple, this worked divinely with little to no downsides. When the 2010s came around, there were many specialised frameworks and programming languages, and marketing teams wanted many new features quickly developed, still maintaining reliability \citep{di2018migrating,Microser52:online}. Even then, if the code base of the application is stored in a single repository, developers have to go through a long process to review and test whether the changes will not break the current systems. Developers are also limited by the frameworks and programming languages that were initially chosen for the project.

To tackle these problems a new way to develop applications was introduced, it's called "Microservices". The idea behind this concept is to break all functionalities of large monolithic applications into small individually scalable services and to give ownership of each service to a clutch of people who work separately. With this flow, developers are free to use whatever tool they like to develop each service. Because these services are developed in parallel by different teams, this increases the development speed by an order of magnitude \citep{Understa56:online}.

As these services are relatively small and tailor-made to run on cloud environments it's easier to take something that's running on the developer's local machine to the production cluster in a matter of minutes. This is mainly due to modern native cloud tools, such as CI / CD pipelines, which automatically build and test the code for them, which can save a lot of time by simply performing repetitive tasks that are prone to human errors \citep{Whataret68:online}.

\subsection{Monitoring Cloud-Native Applications} \label{monitoring-bg}
Even though cloud-native applications have a lot to offer when it comes to developer velocity and productivity, It has their fair share of issues. Most of these problems are linked to the sheer complexity of these systems and not having a proper way to monitor them \citep{5WaysYou35:online}. All three major cloud providers provide a way to monitor these applications efficiently and some great open-source projects do this well, But to take full advantage of those systems, developers have to adapt their services to export all the vitals in a way the monitoring system understands. This works for most parts and is what all large companies are implementing; even if it takes more developer time, it is ultimately significant in regard to disaster recovery.

But there is still a slight problem with this approach. Once the system starts to scale up to hundreds of services, the number of vitals that has to be monitored goes to thousands and will require a lot of additional \acp{sres} and will have drop to lot of non-crucial service vitals and derive abstract \acp{sli} to make it \textbf{humanly} possible to understand the current state of the system.\\
