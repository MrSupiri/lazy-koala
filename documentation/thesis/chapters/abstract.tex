\chapter*{Abstract}

Cloud computing has shown a considerable growth in the past few years, due to its scalability and convenience. With this change, a new programming paradigm called cloud-native was originated. Cloud-native applications are often developed as a set of stand-alone microservices yet could depend on each other to provide a unified experience. Although microservices introduce many benefits when it comes to flexibility and scalability, it could be a great affliction to operate in production. Specifically, when operating a large system with hundreds of microservices interacting with each other, even the smallest problem could result in failures throughout the system.

% Cloud computing is a steady rise for the past few years due to its scalability and ease of use. With this change, a new programming paradigm called cloud-native was born. Cloud-native applications are often developed as a set of stand-alone microservices yet, it could depend on each other to provide a unified experience. 

% This helps different teams to work on different services which increases the development velocity. This works well for medium to large companies but over time this mesh of services could become very complicated to a point where it's very difficult for a single person to understand the entire system. When the system consists of thousands of individual services talking and depending on each other, the network layer of that system becomes chaotic. A failure in a single point could create a ripple effect across the entire system. When something like that happens it could take a considerable amount of time to zero in on the exact point of the failure.

The foci of this project are twofold. First, the authors introduce a robust Kubernetes native toolkit that helps both researchers and developers collect and process service telemetry data with zero user instrumentation. Secondly, the authors proposed a novel way to detect anomalies by encoding raw metric data into an image-like structure and using a convolutional autoencoder to acquire the knowledge of the general data distribution for each service and to detect outliers. Finally, a directed graph was used along with anomaly scores calculated before were used to visually show the spread of an anomaly to the user. 

After an extensive testing and evaluation process, it was found that the telemetry extraction components are both resilient and lightweight even under sustained load, while the anomaly prediction algorithm is accurate and generalizable. 
\newline
\newline
\textbf{Keywords}:
AIOps, Monitoring, Disaster Recovery, eBPF, Kubernetes
\newline
\textbf{Subject Descriptors}:
• Computing methodologies $\rightarrow$ Machine learning $\rightarrow$ Learning paradigms $\rightarrow$ Unsupervised learning $\rightarrow$ Anomaly detection • Computer systems organization $\rightarrow$ Architectures $\rightarrow$ Distributed architectures $\rightarrow$ Cloud computing