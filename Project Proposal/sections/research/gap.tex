
{\let\clearpage\relax\chapter{Research Gap}}

After a literature survey author came conclusion finding a root cause of any failure within a distributed system is a very difficult issue due to it not having single output we can try to predict and most researchers have built their own simulation of a distributed system by themselves since there isn't any open dataset about monitoring data mainly because it could contain sensitive information. 

Most currently established researches are done towards creating statistical models like clustering and linear regression. Even though these algorithms perform very well in small-scale systems, they struggle to keep up when the monitoring data become very noisy with scale. Another problem none of these papers properly  addressed was constant changes to services. All most published research considers target services as static but in reality, these services can change even many times per day \citep{GoingtoM51:online}.

After talking with industry experts author concluded three main issues all had with using a machine learning model as monitoring agent Reliability, Interpretability, and Tunability. On reliability, experts said too many false positives will make operators lose faith in the system because it's gonna be another distraction to them. As the operators have to take critical decisions with the output of these models, it has been interpretable by humans \citep{ribeiro2016should}. Finally, this system should act more like a tool rather than a replacement to human operators, because no matter machine learning models cannot compete with the context a human can handle. 
